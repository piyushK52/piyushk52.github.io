<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://piyushk.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://piyushk.github.io/" rel="alternate" type="text/html" /><updated>2024-07-07T12:09:51+05:30</updated><id>https://piyushk.github.io/feed.xml</id><title type="html">Learning Repository</title><subtitle>Personal thoughts on what I see around. </subtitle><entry><title type="html">Designing a good enough Discord bot</title><link href="https://piyushk.github.io/jekyll/update/2024/07/06/discord-bot.html" rel="alternate" type="text/html" title="Designing a good enough Discord bot" /><published>2024-07-06T20:44:32+05:30</published><updated>2024-07-06T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/07/06/discord-bot</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/07/06/discord-bot.html"><![CDATA[<p>Recently, I worked on a Discord bot that people can use to generate AI images and videos. Here, I will share a quick overview of its system design.</p>

<div style="text-align: center;">
  <img src="/asset/images/discord_bot.png" alt="Discord bot" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>The Discord client and most of the FastAPI processes are quite fast, involving only some database operations. The GPU load has been offloaded to third-party APIs, and the results are received through webhooks. However, I noticed some slowdown when multiple users started using the bot simultaneously. Upon further investigation, I realized that only a couple of methods were the bottlenecks in the process, such as image_transform (which downloads ~20 images, resizes them, crops them, and finally uploads them). I shifted all these CPU-intensive methods to Lambda functions. This greatly improved the speed and decreased the load on the VM. Now that both CPU and GPU-intensive processes are highly scalable, the bot can easily handle thousands of users.</p>

<p>The bot structure was also made modular, using cogs for individual commands. Since Discord client threads have a connection timeout, most of the client calls returned immediately from the backend, and the tasks were added in the background. Sample code is shown below.</p>

<div style="text-align: center;">
  <img src="/asset/images/bot_fastapi_code.png" alt="Sample code" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>To scale this to millions of users, one would need to shard the client instance across multiple VMs, preferably in an ECS cluster. Similarly, the backend would need to be deployed in an ECS cluster.</p>

<p>As the bot created and stored numerous images and videos, we implemented a simple cron job to schedule S3 bucket cleanup. Whenever a file was created, we injected expiry_time metadata into it, which the cleanup job used to determine whether to delete the file or not.</p>

<div style="text-align: center;">
  <img src="/asset/images/lambda_cleanup.png" alt="Cleanup" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Recently, I worked on a Discord bot that people can use to generate AI images and videos. Here, I will share a quick overview of its system design.]]></summary></entry><entry><title type="html">The Next Evolution of Generative AI Tools</title><link href="https://piyushk.github.io/jekyll/update/2024/05/11/ai-tools.html" rel="alternate" type="text/html" title="The Next Evolution of Generative AI Tools" /><published>2024-05-11T20:44:32+05:30</published><updated>2024-05-11T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/05/11/ai-tools</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/05/11/ai-tools.html"><![CDATA[<p>This post explores the different tools in the generative AI landscape and what’s next.</p>

<div style="text-align: center;">
  <img src="/asset/images/tool_banner.webp" alt="AI Tools" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Release of the Stable Diffusion 1.5 kicked off a frenzy even among non-ai people as being able to generate images in a single click seemed just like magic. There were a couple of python scripts initially but the need for a proper tool led to SD webui/a1111 (made by a1111).</p>

<p><br /></p>
<h2 class="post-list-heading">Automatic1111</h2>

<p>A1111 was the first tool that got mass adoption in the AI community, in fact in the SD subreddit it was the only tool being used/discussed for a couple of months. Since there was a strong community effort behind it, a lot of extensions were developed for it and there were a lot PRs being made to improve features. People were experimenting with different things and sometimes the things that worked seemed counterintuitive to even the ML folks. I remember many people were asking for the “clip skip” feature as it is very useful for anime related model (models based on NAI) but in some Github discussions the maintainers were like “huh? why do you wanna skip clip layers, it makes no sense”. It is one of those things where we really need the entire community effort for it to work. Obviously now, it makes more sense and it is like a standard option in almost every tool. People were adding all kinds of extensions as well, like Ebsynth which would have been very tedious to setup if someone was using just Python scripts.</p>

<div style="text-align: center;">
  <img src="/asset/images/a1111.png" alt="A1111" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>A1111 is still used widely but it is in a slow decline and the main reason for it is that it was unable to capture the entire generative AI ecosystem. It was very good for tinkerers, those who want to add small features/extensions and decent for artists as they can experiment with a lot of settings but not really great for developers as there was a lot of inflexible code and the app is not built in modular way where different components can be added and removed to create a new “workflow”, like first do txt2img then img2vid then upscale.. all this was manual.</p>

<div style="text-align: center;">
  <img src="/asset/images/a1111_venn.png" alt="A1111" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p><br /></p>
<h2 class="post-list-heading">ComfyUI</h2>

<p>ComfyUI was a major step up from A1111, not only did it have a much slicker js interface it improved on a lot of issues that plagued A1111. It allowed for creation of proper workflows and it was much more developer friendly compared to A1111 which meant a LOT of custom nodes were (and still are) being built daily. Creating a workflow and being able to share it is an insanely powerful feature in itself that accelerated it’s adoption. You will find many people (including myself) opening tickets inside new ML repos asking for them to port it to ComfyUI. In a sense, it managed to capture both the tinkerers and developers completely, but there is still a large part of the artist community that needs to be pulled into the AI art revolution. One other drawback of Comfy is that since it is managed in a very decentralised way, breaking of flows is common. For e.g. a couple of days ago the Derfuu Integer node was removed from the Comfy Manager (that is managed by some other individual) and it broke a lot of workflows instantly. Also, there are a lot of nodes that are basically direct wrappers on top of the diffusers code that adds to the inefficiencies and there are no stable releases (as everyone is making their own release).</p>

<div style="text-align: center;">
  <img src="/asset/images/comfyui_ss.png" alt="ComfyUI" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<div style="text-align: center;">
  <img src="/asset/images/comfy_venn.png" alt="ComfyUI" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Many companies and startups have started cloning the node based structure of ComfyUI and making incremental adjustments to it. Also there are other tools similar to Comfy but with a very robust codebase, like InvokeAI. I think because of how InvokeAI is re-imagining the node based tools, it can be very powerful and may become the dominant player. As it also provides a minimal interface for artists who are not very familiar with the node based structure.</p>

<p>Dough was an attempt by us (Banodoco) to provide a set steps for video generation. Images &gt; Customize Images &gt; Arrange in shots &gt; Generate video. Although it is very good for video generation use cases, it fails in roping in the devs and tinkerers, also having limited options for generation limits artist (although that will be fixed soon!). Leaving out even one group has a big impact on the adoption, but it is something that we will fix in it’s next iteration.</p>

<div style="text-align: center;">
  <img src="/asset/images/dough_venn.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p><br /></p>
<h2 class="post-list-heading">What's Next?</h2>

<p>I believe the companies banking on ComfyUI ecosystem will have a tough time in the near future because it’s adoption will slowly decrease. Just like A1111 hosting services are far and in-between, ComfyUI workflow hostings will vanish. The next iteration of tools (or tool) will have the following characteristic</p>
<ol>
  <li>It will be open source and community-driven but still a focused effort and would have some kind of community-driven gatekeeping for what to let in (in terms of PRs/functionalities)</li>
  <li>Stable code releases including all the bells and whistles</li>
  <li>A UI that allows creating processes on top of the workflows</li>
  <li>It is cohesive and feels like a native app, if people want they go into the noodle-graph land or directly into the code but overall the app provides multiple levels of abstraction</li>
</ol>

<p>What this tool will look like? I don’t know but hopefully we will have something very close to it pretty soon!</p>

<div style="text-align: center;">
  <img src="/asset/images/next_gen_venn.png" alt="Next gen tool" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post explores the different tools in the generative AI landscape and what’s next.]]></summary></entry><entry><title type="html">Dough: Low level design</title><link href="https://piyushk.github.io/jekyll/update/2024/05/11/dough-lld.html" rel="alternate" type="text/html" title="Dough: Low level design" /><published>2024-05-11T20:44:32+05:30</published><updated>2024-05-11T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/05/11/dough-lld</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/05/11/dough-lld.html"><![CDATA[<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/dough_0.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>The AI art tool that I have been working for quite a while now is finally released in beta! You can try it here - <a href="https://github.com/banodoco/Dough">Dough Beta</a> . Pom provided a lot of inputs and tirelessly worked with the tool to refine the layout/functioning. I was handling most of the technical aspects and in this post I will share a low level design of Dough.</p>

<p>Firstly, from the inception I tried to keep Dough as modular as possible. Different components and integrations can be swapped easily and developed independently. At the center of the app is the core Streamlit UI and related utility functions. These interact with the data layer, that is essentially the backend, that can be easily plugged into multiple backends. In fact we had different backends for the local and hosted version (obviously!). Different backends can have their own databases and logic for maintaining users and payments.</p>

<div style="text-align: center;">
  <img src="/asset/images/dough_1.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>As Streamlit refreshes the entire app (every single line!) anytime a state change occurs it becomes super hard to integrate APIs with it as we can’t have the app calling 30 different APIs on a single dropdown click. A simple solution for this was to cache as much as possible. I created a simple cache decorator that was placed on top of the DataRepo (entrypoint to the data layer). This way it could cache any backend connected to it, local or hosted. Below you can see a simple function inside the decorator, which first checks the cache and if the data is not found then it hits the API.</p>

<div style="text-align: center;">
  <img src="/asset/images/dough_2.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/dough_3.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Now having just used the A1111 there was an obvious issue, people wanted to host it but it wasn’t built for a new backend integration, the UI layer was tighly coupled with the ML layer and thus many startups trying to deploy it were deploying it in a completely new container everytime. This meant a lot of GPU time was wasted where people were just playing with the UI. Also everything was synchronous, which means that the UI will be blocked until the generation completes. We planned on making the operations asynchronous, which means the user is free to interact with the UI while the generation runs in the background. To solve both of these issues I made a modular ML layer and a background runner that schedules the tasks and updates the generation status. Having this modularity means anyone can plug in their own ML processor, we used to have third-party APIs but were able to quickly add ComfyUI support for local generations because of this modularity.</p>

<div style="text-align: center;">
  <img src="/asset/images/dough_4.png" alt="Notification History" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>There are still a lot of improvements to be done and features to be added. The technical side is just one aspect of the app, Pom’s push for perfection and insights in UX/functionality has been monumental in it’s development. We plan on making a much slicker and fluid version in a JS framework very soon!</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Artifact: social with news &amp;gt; news with social</title><link href="https://piyushk.github.io/jekyll/update/2024/01/13/Artifact.html" rel="alternate" type="text/html" title="Artifact: social with news &amp;gt; news with social" /><published>2024-01-13T20:44:32+05:30</published><updated>2024-01-13T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/01/13/Artifact</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/01/13/Artifact.html"><![CDATA[<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/artifact_closing.png" alt="Artifact Closing" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>As someone who has always been interested in the consumer social sector, I have been following Artifact’s progress from its launch. I was intrigued by the idea of mixing news with powerful ML recommendations, especially coming from people who understood this segment better than anyone. One example that Kevin Systrom gave in one of his interviews was that TikTok mixed videos with the most powerful ML recommender system and it completely changed the video consumption habits of the people. He believed that the new generation of products would have the traditional delivery mechanism but would be delivered in a highly personalized way. That’s very smart considering in the case of news the more polarizing it is the faster it spreads, and in the case of personalization, it can be made to appeal to different niches.</p>

<p>Here are some of my thoughts on what I think worked for them and what didn’t. First of all, what worked for them? I think they had a very strong sense of app features that give dopamine hits, e.g. when you share a link to some news article and someone opens it, you get a notification that someone viewed the link you opened. They added a very nice feature, the “clickbait or not” flag. The app was super smooth and never felt like an MVP of any kind.</p>

<p>Now for the things they got wrong. First, I think they were not able to nail the virality aspect of the news. Here, is a screenshot of notifications of Artifact vs Inshorts (a popular news app in India). I think their algorithm optimized for utility over virality. In the beginning, I feel they should have covered some sensational topics to get some eyeballs and have their comment sections filled with discussions. The main advantage of this would have been that they would have increased their install base.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/artifact_notifications.png" alt="Notification History" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Secondly, I feel one major issue with them was that they couldn’t distinguish themselves from the competition. News summary? I have already been using an app for that for the past 8 years. Comments/reactions in the news? there are tons of apps for that. There was simply nothing that made them stand out. I am a tech enthusiast therefore I had the app installed, but no one else in my circle was using it. Thirdly, I think they took the wrong approach by trying to mix social aspects in a text news app when there are already text social media with news aspects (Twitter). In social media, if you like a dance video, the influencer can create 10 more videos in a day to keep you glued to the screen but it’s not the case with news, you can’t make 10 more news similar to what I like out of thin air. This means the ‘content’ generation is a huge problem in news and that’s why almost all the major players rely on sensationalism/dramatization.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/jeremy_corbell.png" alt="Clickbait" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>I believe one of the mistakes they made was to fundamentally confuse the nature of news consumption in today’s world. First, is of course the consumption of sensational news stories but the second is creating engaging content around it. Social media can magically create content around the same news and have you watching the same thing for hours, how? For. e.g. Biden slips while climbing a staircase. In a news app, this would be just one notification (maybe not even that) but on a social media platform, you can have hours of content on it from memes to critic videos. One other reason why social media dominates the news space more than the news itself is because that is where news gets created (not all but many).</p>

<p>I feel people will naturally gravitate towards the niche they like but they won’t consume text content with ML recommendation. They feel much more comfortable watching a random dude dramatize the whole thing and create an echo chamber for them. Since the mainstream media has lost all its credibility people turn to influencers (who are in many cases subject experts) to seek news and that’s why we see the rise of YouTube channels such as “China uncensored”. Also, in my opinion, news, however engaging, is not something that people will consume continuously and that’s also why they prefer social media because it gives them a mixture of content to consume news, sports, dumb fun, gaming, etc.. Overall, I think it was somewhat inevitable that Artifact was going to have a tough time with its initial concept as its founders said that they were unsure during its development phase if it would work or not.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Deploying a Streamlit app on production</title><link href="https://piyushk.github.io/jekyll/update/2023/12/30/streamlit-system-design.html" rel="alternate" type="text/html" title="Deploying a Streamlit app on production" /><published>2023-12-30T20:44:32+05:30</published><updated>2023-12-30T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2023/12/30/streamlit-system-design</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2023/12/30/streamlit-system-design.html"><![CDATA[<p>I have been working on an open source AI video generator for quite some time now. Though it’s a simple tool, it being built in Streamlit posed some challenges both in it’s internal structuring and deployment. Here, I will share some of the problems faced by me and how I solved them.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/app_screenshot.png" alt="App Screenshot" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
  <figcaption>App Screenshot (WIP)</figcaption>
</div>
<p><br /></p>

<p>The very first thing I did to make the app scalable was to deploy it on an ECS cluster, which depending on the load will auto-scale the number of containers running. The problem with this approach is that AWS load balancer auto routes the incoming requests to containers with low load whenever the number of containers changes. Since Streamlit is server side rendered and it maintains the state locally (on the server), this switch will kill the app functioning, especially in our case since we are keeping a large amount of data in the Streamlit session state for quick reloads. After some searching I found that AWS provides the option to enable sticky sessions in ecs target group alb which maintains the user’s connection to a specific ECS container (connection is identified by browser cookies). This setting can be enabled by adding the following terraform code inside aws_alb_target_group.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/sticky_session.png" alt="Sticky session" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>After this the problem that we encountered a lot with the app was speed. Since, Streamlit is somewhat like Flutter in the regards that it refreshes the entire app on every little change, our app was getting very slow, since we were making quite a few API calls to the backend. The solution to this required changing the deployment options and the internal app structure. In the case of deployment I noticed that the DNS resolution and the TLS handshake were taking a significant amount of time. Since the user will never see any of the https calls (Streamlit directly sends the rendered page to the end user) I can just use http instead of https. Also instead of going over the internet and doing DNS resolution, I created an internal private DNS which the Streamlit app can use to quickly connect to the backend. This more than doubled the API interaction speed.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/tls.webp" alt="API timing" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
  <figcaption>API Timings</figcaption>
</div>
<p><br /></p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/ai_app_design.png" alt="High level system design" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
  <figcaption>High level system arch</figcaption>
</div>
<p><br /></p>

<p>Now on the app side I tried to cache as many data models as I could. We built a simple wrapper on top of our API repository, which cached the API results in the session state. The caching logic we used was very simple, we stored the API data as they came and whenever a particular model was altered (updated/deleted) we refreshed all the related entities. This again gave us a significant speed boost. We are still looking for ways to make it even faster, but not having a proper state management solution in Streamlit creates some challenges.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I have been working on an open source AI video generator for quite some time now. Though it’s a simple tool, it being built in Streamlit posed some challenges both in it’s internal structuring and deployment. Here, I will share some of the problems faced by me and how I solved them.]]></summary></entry><entry><title type="html">Generative AI: A semi-technical look</title><link href="https://piyushk.github.io/jekyll/update/2023/07/19/generative_ai.html" rel="alternate" type="text/html" title="Generative AI: A semi-technical look" /><published>2023-07-19T21:54:32+05:30</published><updated>2023-07-19T21:54:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2023/07/19/generative_ai</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2023/07/19/generative_ai.html"><![CDATA[<p>I have been quite busy since my last post. In this post, I will cover my experience with generative AI and what I think about it from a technical perspective being a software developer. I worked on multiple personal projects, some complete, some incomplete, and some still in progress. I will start with the LLMs as it was ChatGPT that took the internet by storm. Though AI was here long ago, it was the first time that people could use it naturally with the chat interface. Having opened their API to the public everyone started building wrappers on top of it. To me as well it looked like the right opportunity to ride the AI wave or at least make something worthwhile. So, I started with the problem most software engineers are troubled by, coding. I wanted to make a program that will automatically write the code based on your existing code base and development practices. This is a high-level overview of what I thought would work.</p>

<p><img src="/asset/images/auto_code.png" alt="Auto code genrator" /></p>

<p>And this was the result</p>

<video width="100%" preload="auto" muted="" controls="" style="margin: auto;">
    <source src="/asset/videos/code_generator.mp4" type="video/mp4" />
</video>
<p><br /></p>

<p>The results were not that great. Although ChatGPT was good at giving small code snippets, writing the entire code seemed very far fetched (I believe it still is). The chaining of multiple commands utilizing CoT and ReAct added more to the problem as a single deviation/error at one of the steps compounded heavily till the program reached the end. I tried to minimize the error in multiple ways but had no luck so I gave up. I looked at other impressive demos on Twitter but none had a good working product and everyone had the same issue. I believe even with GPT-5 coding the entire codebase is an extremely far fetched scenario. Maybe this was the major breakthrough that we would see even in the next 5-7 years.</p>

<p>From LLMs I switched my focus to image generation as it impressed me much more than the LLMs. I tried the base SD in the very beginning and it was super bad but when I tried Midjourney I was just blown away. I have been an admirer of art (especially fantasy art) for a long time, and seeing a computer generate such amazing images in just seconds was a surreal experience. I tried a bunch of custom models and quickly created a website where people can generate images. I also tried a bunch of other things to see what sticks, but the pace of development in AI space was so fast, with players picking millions in funding that it was almost impossible to keep up. A simple example would be Prompthunt and similar sites, which started by selling prompts but their business model was soon invalidated when both Stable Diffusion and Midjourney launched their prompt predictor tool (Clip and /describe). Here’s the site that I made, I am still experimenting with different things on this and hoping I find the right hook to monetize this somehow.</p>

<p><br />
<img src="/asset/images/promptpedia.png" alt="PromptPedia" />
<br />
<br /></p>

<p>Custom models generated amazing results while Midjourney is in a league of its own. Midjourney reaching $1 billion ARR shows just how much of a big deal this was. But one major issue I felt with image generation was the loss of control and I think text-to-image is not the right way to go about it. A different way to think about this is the data filling, we provide some text which is a couple of KBs, and the AI model generates an image that is a couple of MBs. This thousandfold data creation is largely driven by the model’s training and there is very little control we have over it. For example, I wanted to make an image of a fantasy land in a dark setting with rocks flying. I already had its image by some artist but wanted to make it even better. No matter what I did it was almost impossible to get what I want, the only solution that could work in these scenarios is to draw the image from scratch by hand. AI image models are still a black box and though you can still achieve a lot by proper prompt engineering and other control techniques (like controlnet, image-to-image, and parameter tuning) it’s nowhere close to artists creating art from scratch.</p>

<p>I next started experimenting with AI video generation. At the time there were just some basic demos out there by Runway but nothing concrete. A lot of people on Reddit and Twitter were generating videos by converting every single frame than removing the noise artifacts. But in my opinion, these simple restyling can be better done by static programs like EBsynth. I tried a lot of different things, the main ones being multi controlnet and image-to-image at different settings but couldn’t get a good result. The only way that seemed to work was to train an SD model on both character and style of the particular character that you wish to transform and then apply multi-controlnet along with the custom model that you created. Of course, this makes very little practical sense and thus still I have no good way to make this work. Check one of the results that I generated below.</p>

<p><br /></p>
<video width="80%" preload="auto" muted="" controls="" style="margin: auto;">
    <source src="/asset/videos/ai_vid_to_vid.mp4" type="video/mp4" />
</video>
<p><br /></p>

<p>I believe there is still significant research and development left in the AI space. Though the results are impressive and can perform very well on small tasks they are still far from practical use cases and nowhere near what the AI doomsayer would have us believe. I will keep experimenting with this stuff and update it here. The quote from Yann LeCun, Chief AI scientist at Meta summarizes this perfectly.</p>

<p><br />
<img src="/asset/images/yann.png" alt="Yann LeCun Tweet" />
<br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I have been quite busy since my last post. In this post, I will cover my experience with generative AI and what I think about it from a technical perspective being a software developer. I worked on multiple personal projects, some complete, some incomplete, and some still in progress. I will start with the LLMs as it was ChatGPT that took the internet by storm. Though AI was here long ago, it was the first time that people could use it naturally with the chat interface. Having opened their API to the public everyone started building wrappers on top of it. To me as well it looked like the right opportunity to ride the AI wave or at least make something worthwhile. So, I started with the problem most software engineers are troubled by, coding. I wanted to make a program that will automatically write the code based on your existing code base and development practices. This is a high-level overview of what I thought would work.]]></summary></entry><entry><title type="html">Thoughts on the metaverse</title><link href="https://piyushk.github.io/jekyll/update/2023/04/17/metaverse.html" rel="alternate" type="text/html" title="Thoughts on the metaverse" /><published>2023-04-17T21:54:32+05:30</published><updated>2023-04-17T21:54:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2023/04/17/metaverse</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2023/04/17/metaverse.html"><![CDATA[<p>I have been using the Oculus Quest 2, on and off for about 1 year now. The first time I used it I was blown away. I thought to myself, this is definitely the future. Weirdly enough, when I discussed it with my friends and even the reviews online, no one particularly seems to enjoy it. “It’s a poorly designed gaming system” - this is the typical response that I got. Later in this post, I will delve into potential enhancements for broader adoption, as well as fundamental paradigm shifts that may be necessary.</p>

<p>The virtual and augmented reality (AR/VR) industry has plummeted from investors’ radar due to lackluster sales figures of Meta and with highly anticipated startups like Magic Leap failing to meet expectations by a significant margin. Top tech personalities like Elon Musk have also said that “I don’t see someone strapping a friggin’ screen to their face all day”. John Carmack who was in charge of building the metaverse left Meta saying that the pace of innovation is slow.</p>

<p><br /></p>
<video width="80%" preload="auto" muted="" controls="" style="margin: auto;">
    <source src="/asset/videos/bigscreen_ss.mp4" type="video/mp4" />
</video>
<p><br /></p>

<p>This is a short screen capture that I did in the VR and it felt like I was really there. The discussions in VR are very different from what one experiences in group chats or group calls. There really is an immersive feel to it. But it comes at a cost, when you enter the metaverse you become completely disconnected from the actual world. So unlike on a group video call, you can’t quickly share/access your files on your computer/mobile or the internet. You can’t hear your mom yelling for you to come down. For a little amount of immersion, it feels that we sacrifice a large amount of control and awareness. This can be both good and bad depending on the circumstances, but ideally, it’s good for things that require less control. For e.g. watching movies, sitting in a group and just discussing random topics, or playing a game where all the controls are already present in the VR. For most of the day-to-day work, it would be difficult to migrate to VR.</p>

<p>One other issue is that very few of your friends are in it. Even if they are, there are very few things to be done. We already have MMORPGs and similar games, where your network is present and the ‘immersive’ experience doesn’t make much difference. As stated in “The cold start problem”, the goal should be to first build an atomic network and then grow from there. I believe Meta had the right idea, of focusing on creating VR-first experiences. It can be a game but mostly has to be something different. Maybe it’s a field trip to an alien planet with your friends, maybe it’s an escape room, or a VR documentary. I believe BigScreenVR has found its core group of users (like me), who tune in every now and then. What’s their secret sauce? They let anyone host a private room and share any video. So you could in ‘theory’ have ‘some’ people screening the latest movies and you get to watch it from a perfect spot in a grand theatre without a kid crying in the back.</p>

<p>User experience is the other thing that creates a lot of friction. First, you have to put on the clunky headsets in position then turn them on then wait for them to boot. If in between you have to go to the bathroom, you have to take the entire thing off (headsets, controllers, charging cables, headphones, and wires) and then put it back on when you come back. The headset should be small, super quick to boot (or maybe should plug in directly to your laptop), have a long battery life or charges through the laptop connection, and has just simple gloves for hand tracking (no turning on or battery required for those). I believe one other limitation is the technology, current graphics look like they are from the PS2 era. As Apple has been always great at striking the balance between tech and user experience, I believe they have the most chance of nailing this correctly. We will have to wait and see what Apple unveils in June.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I have been using the Oculus Quest 2, on and off for about 1 year now. The first time I used it I was blown away. I thought to myself, this is definitely the future. Weirdly enough, when I discussed it with my friends and even the reviews online, no one particularly seems to enjoy it. “It’s a poorly designed gaming system” - this is the typical response that I got. Later in this post, I will delve into potential enhancements for broader adoption, as well as fundamental paradigm shifts that may be necessary.]]></summary></entry><entry><title type="html">System Design of my social media app</title><link href="https://piyushk.github.io/jekyll/update/2023/04/12/unfiltr_system_design.html" rel="alternate" type="text/html" title="System Design of my social media app" /><published>2023-04-12T22:55:00+05:30</published><updated>2023-04-12T22:55:00+05:30</updated><id>https://piyushk.github.io/jekyll/update/2023/04/12/unfiltr_system_design</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2023/04/12/unfiltr_system_design.html"><![CDATA[<p>Last year me and my friend started a startup. We started with a BNPL app but shortly after that the revised RBI guidelines came to effect and all our plans were thrown into disarray and we had to pivot to a different idea altogether. In a desperate attempt to catch the billion-dollar wave of BeReal, we pivoted to make a BeReal clone for the Indian market (I know this was one of the worst pivots a startup with our circumstances can take). As we started building the app I realized it was not as simple as it seems. Since I was leading the tech, I had to figure out a lot of the stuff. In this article, I will summarize the high-level system design that I followed to build our MVP.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/unfiltr_2.webp" alt="App Screenshot" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Let’s start with the tech challenges that we needed to solve.</p>
<ul>
  <li>Feed should load as quickly as possible</li>
  <li>Reactions and comments should also be visible with minimal delay</li>
  <li>Feed should be customized per user (just basic personalization for now)</li>
</ul>

<p>Remember, this was just an MVP and we only had about a month and a half to roll everything out, so some of the solutions might seem a little hackish but it worked for us. Since this was a read-heavy system we cached most of the stuff. All the posts were updated in follower feeds and directly served from the cache after a couple of joins.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/tweet_normal_user.png" alt="Tweet update" style="height: 100%; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>One drawback with this approach is that if a user has a large number of followers this update can be slow. Therefore to tackle that, what we did was to update the celebrity posts at the time of fetching the user feed. We checked what celebrities the user is following and then added their most recent post in the user feed. In larger systems, there might be a completely separate logic for handling which post should be added (maybe it’s the most interesting to the user or with the most number of likes or a mix of the two).</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/celebrity_user.png" alt="Tweet update" style="height: 100%; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>For storing reactions and comments we used the same formula. We cached most of the stuff. Whenever an update was made we updated both the SQL and the cache. Normally when the system scales, these SQL updates will need to be batched or else it will overtax the DB. We followed a simple schema for storing these.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/reaction_cache_schema.png" alt="Cache Schema" style="height: 100%; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Now for customizing the main user feed we kept it simple. Ideally, different ML pipelines work over the interaction data of the user along with other parameters to achieve this. But we just gave a score to different interaction events like tap, share, comment, and like and generated a rank based on the weighted score (just making an MVP here). The interaction data was put in a Kinesis stream and then in intervals of 30 minutes, the data was batched and written into S3 files. Our backend would pick this data at regular intervals and update the post scores.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/kinesis_data.png" alt="Kinesis update" style="height: 100%; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Apart from this, there were other optimizations like batching the notifications when it exceeds a particular threshold, converting profile images to lower resolution for loading thumbnails, making a Jenkins task trigger at a random time, etc. To keep things streamlined we used Terraform for managing the infra. This was my second time working this it and I was really impressed by how much efficient it was compared to manually tinkering with the GUI.</p>

<p>Though I enjoyed working on the project, we couldn’t make it grow. Social media is one of my favorite fields to work in and I hope I get a chance to make something worthwhile in the future.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Last year me and my friend started a startup. We started with a BNPL app but shortly after that the revised RBI guidelines came to effect and all our plans were thrown into disarray and we had to pivot to a different idea altogether. In a desperate attempt to catch the billion-dollar wave of BeReal, we pivoted to make a BeReal clone for the Indian market (I know this was one of the worst pivots a startup with our circumstances can take). As we started building the app I realized it was not as simple as it seems. Since I was leading the tech, I had to figure out a lot of the stuff. In this article, I will summarize the high-level system design that I followed to build our MVP.]]></summary></entry><entry><title type="html">Race for an AI marketplace</title><link href="https://piyushk.github.io/jekyll/update/2023/04/07/race-for-ai.html" rel="alternate" type="text/html" title="Race for an AI marketplace" /><published>2023-04-07T20:44:32+05:30</published><updated>2023-04-07T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2023/04/07/race-for-ai</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2023/04/07/race-for-ai.html"><![CDATA[<p>AI research is going so fast that even the adult industry is having a tough time catching up. Every day there’s a brand new paper on different AI architecture and yesterday I saw a paper by HuggingFace for a model that can communicate with all the different models present on the platform and with time will learn what model to use with what query. It’s moving fast enough to convince many top tech people that we will reach AGI in the next 5 years! that’s wild. Although I feel we might still have a decade or so before we reach there, but we can definitely get pretty close in the next couple of years.</p>

<p>So what’s next in AI? I believe the next goal is to make it much more integrated into our life. We want AI to take care of our finances, groceries, research, entertainment, and life in general. We want a PERSONAL ASSISTANT in our pocket. There will be a unified layer that will communicate through all the different oracles and entities on our behalf of us. For this layer to be personal and intimate, it can’t be just any other piece of technology it has to feel personal. OpenAI has figured this out and is going for the takedown.</p>

<p><img src="/asset/images/ai_landscape.png" alt="AI Landscape" /></p>

<p>You must be thinking, but wait a minute OpenAI doesn’t have an AI with personality. I think it’s going to have one very soon. Look at the tweet below.</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Startups developing AI characters lookout. Filled this OpenAI survey and I think I know what&#39;s next. <a href="https://t.co/uStZNpLLUg">pic.twitter.com/uStZNpLLUg</a></p>&mdash; Piyush Kumar (@thisispiyushK) <a href="https://twitter.com/thisispiyushK/status/1644247755483754496?ref_src=twsrc%5Etfw">April 7, 2023</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>Obviously, there won’t be just one company with this powerful AI, and seeing the pace of development it is very much possible that developers would be able to create their own AI for niche use cases. The good strategy therefore could be to aggregate these niche AIs on a single platform. This marketplace won’t be competing with the likes of OpenAI, Meta, or Google. Quora released it’s own platform <a href="https://techcrunch.com/2023/02/06/quora-opens-its-new-ai-chatbot-app-poe-to-the-general-public/">Poe</a> a couple of months back and recently <a href="https://www.fixie.ai/">Fixie</a>, a startup in the similar direction picked up $17 million in funding</p>

<p>Who wins the AI wars is yet to be seen. It could very easily be the case that going above the current reasoning capacity of GPT-4 is not even possible with neural nets or that the entire internet’s data is not enough to train the next big AI. I will be following this space and will be sharing some small projects that I am building along the way.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[AI research is going so fast that even the adult industry is having a tough time catching up. Every day there’s a brand new paper on different AI architecture and yesterday I saw a paper by HuggingFace for a model that can communicate with all the different models present on the platform and with time will learn what model to use with what query. It’s moving fast enough to convince many top tech people that we will reach AGI in the next 5 years! that’s wild. Although I feel we might still have a decade or so before we reach there, but we can definitely get pretty close in the next couple of years.]]></summary></entry><entry><title type="html">Attention is all you need</title><link href="https://piyushk.github.io/jekyll/update/2023/04/01/attention-is-all-you-need.html" rel="alternate" type="text/html" title="Attention is all you need" /><published>2023-04-01T20:44:32+05:30</published><updated>2023-04-01T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2023/04/01/attention-is-all-you-need</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2023/04/01/attention-is-all-you-need.html"><![CDATA[<p>With tech tutorials flooding the streaming platforms and every next person you meet being a software developer (or planning to become one),
technology is getting commoditized. At least the ones over which the current multi-billion dollar platforms are built upon. We find ourselves in a
sea of open-source projects, all built with the same building blocks in some sense. It didn’t take LLMs long to figure out the patterns
and start coding in a near-human level capacity. The future may still be a little further when anyone who wants to code, can code. But we are
almost there, as almost anyone who wants to code can code and build products right now. Knowledge is available for free on the Internet.</p>

<p>Whatever a small team can build, a giant organization or someone else can build very easily as well (with some exceptions). And this is a common practice, follow a small startup, and if the idea sounds interesting enough just copy it. I realized this when I made a small website summarization plugin using OpenAI APIs, and although there were a lot of plugins already available only a few were with the most downloads and were getting shared everywhere. The key is to differentiate. And to differentiate you need attention. If a guy with 100K followers releases a beta, he instantly gets 50-100 customers while a guy with a much better product gets 1-2 and 
they churn out eventually. In this day and age, attention is one of the very strong moats any business can have. Whether it’s a newsletter, YouTube 
channel, or Twitter account, as long as it has the right audience it has a potential revenue stream.</p>

<p>Big businesses (especially big tech) have realized this and are buying media houses left and right. Hustle’s acquisition by Hubspot is an example of this. Although big businesses don’t have to worry too much about generating revenue but having a media arm that connects with the audience in a genuine manner is very important. People see an ad from a mile away. This is the new approach of creating a community, buy an existing one. The smart move for current entrepreneurs I believe too is to create a community that they can monetize or at the very least try to develop a following.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[With tech tutorials flooding the streaming platforms and every next person you meet being a software developer (or planning to become one), technology is getting commoditized. At least the ones over which the current multi-billion dollar platforms are built upon. We find ourselves in a sea of open-source projects, all built with the same building blocks in some sense. It didn’t take LLMs long to figure out the patterns and start coding in a near-human level capacity. The future may still be a little further when anyone who wants to code, can code. But we are almost there, as almost anyone who wants to code can code and build products right now. Knowledge is available for free on the Internet.]]></summary></entry></feed>