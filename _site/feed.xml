<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://piyushk.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://piyushk.github.io/" rel="alternate" type="text/html" /><updated>2024-09-20T23:13:22+05:30</updated><id>https://piyushk.github.io/feed.xml</id><title type="html">Learning Repository</title><subtitle>Personal thoughts on what I see around. </subtitle><entry><title type="html">Inference over distributed GPUs</title><link href="https://piyushk.github.io/jekyll/update/2024/09/19/inf-gpus.html" rel="alternate" type="text/html" title="Inference over distributed GPUs" /><published>2024-09-19T20:44:32+05:30</published><updated>2024-09-19T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/09/19/inf-gpus</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/09/19/inf-gpus.html"><![CDATA[<p>In this post, I will cover a system I built recently to do inference over GPU clusters, specifically the Salad GPU service. To be clear, this post 
is not about using multiple GPUs to perform a single inference but rather an event driven queuing system.</p>

<p>The obvious challenge was that this GPU cluster runs on consumer hardware, people connect their GPUs to Salad and Salad runs the tasks on them. Though this sounds convenient, two drawbacks of this is that firstly the GPUs can go offline anytime (even mid-process) and secondly the internet speeds can vary significantly. First, for queuing and maintaining the inference states I used the CQRS pattern (Command and Query Responsibility Segregation) and the system defined as below</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/inf_backend_arch.png" alt="System architecture" style="max-height: 800px; max-width: 100%; margin: 0 auto;" />
  <figcaption>System architecture</figcaption>
</div>
<p><br /></p>

<p>All the events go in the task mixer and at regular intervals, a homogenised mixture of tasks is pushed in the queue such that no user blocks the processing pipeline. This component also has the ability to set different priority for different users, thus prioritizing the tasks from a particular user over the others. This functionality was suggested by Peter.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/task_mixer.png" alt="Task Mixer" style="max-height: 600px; max-width: 90%; margin: 0 auto;" />
  <figcaption>Task Mixer, mixing user tasks with equal priority</figcaption>
</div>
<p><br /></p>

<p>In our testing we didn’t find the gateway very robust so we flipped the process and made the containers pick the tasks directly from queue. All the outputs/errors are then pushed to the output queue which are picked by the consumer, stored in the db and then processed. For storing the results of each sampling process a modified version of AustinMroz’s checkpointing script was used, so that in the case of disconnection, the new generation will begin from where it left off. One major benefit of this system design is that the third party GPU provider can very easily be switched to something else (including having multiple providers of different kind as well). The containers also have a background download script that parallely downloads multiple files (in chunks) in the background (of files not present in the container) and has the ability to download stuff on priority as and when needed.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/gpu_container.png" alt="GPU container" style="max-height: 600px; max-width: 90%; margin: 0 auto;" />
  <figcaption>GPU container </figcaption>
</div>
<p><br /></p>

<p>All the containers are connected to the scale server, that serves two purposes. Firstly, it scales the number of containers up or down depending upon the load. This scaling involves measuring task load, container capacity, hysteresis period and couple of other parameters. The other important function of this server is to keep a record of all the containers and their active tasks, incase of any disconnections the task is requeued. There is also a check that runs at regular intervals that takes care of many edge cases that could happen. Below is a graph of chaos testing of the system, with the breaks in the graph showing downtime of the scale server.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/chaos_testing.png" alt="Chaos testing" style="max-height: 600px; max-width: 90%; margin: 0 auto;" />
  <figcaption>Chaos testing with interrupting the scale server while dynamically changing the load</figcaption>
</div>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In this post, I will cover a system I built recently to do inference over GPU clusters, specifically the Salad GPU service. To be clear, this post is not about using multiple GPUs to perform a single inference but rather an event driven queuing system.]]></summary></entry><entry><title type="html">Social media in the age of AI</title><link href="https://piyushk.github.io/jekyll/update/2024/08/09/social-media-x-ai.html" rel="alternate" type="text/html" title="Social media in the age of AI" /><published>2024-08-09T20:44:32+05:30</published><updated>2024-08-09T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/08/09/social-media-x-ai</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/08/09/social-media-x-ai.html"><![CDATA[<p>In this post, I will explore how social media is evolving and what its next phase of evolution might be. I’ll focus on mainstream platforms, excluding message boards or local sites (like Niconico). I’m not an expert on this subject, just an avid reader of tech news, and I’m writing this as a quick analysis. Please consider this post in that context.</p>

<h2 id="from-single-to-multi-modality">From Single to Multi-Modality</h2>

<p>When examining the general trend over the years, one thing becomes apparent: all major platforms are moving from single modality to multi-modality. At the start of the social media era, platforms focused on being either text-based, image-based, or different mixes of the two, targeting various demographics. The first major realization for most platforms was that they could easily add other modalities, providing more content variety to their users without significantly affecting the user experience.</p>

<p>The rise of TikTok demonstrated how a specific content type (short videos, in this case) can dramatically affect adoption. Depending on the platform’s structure, this transition isn’t always straightforward. For example, it’s extremely challenging for YouTube to incorporate text elements, and even YouTube Shorts feels out of place in the current UI. Threads is another good example of this difficulty.</p>

<p>Currently, all platforms are inching towards providing as many types of content formats as possible along with a full range of engagement features (likes, comments, badges, shares, etc.). This is one of the primary reasons why it’s incredibly tough to start a new social media platform. Existing players already have strong network effects and cover all content types.</p>

<h2 id="from-known-to-unknown">From Known to Unknown</h2>

<p>Another major shift in social media platforms is that people are moving away from their existing networks to interact with complete strangers. They are seeking novel interactions and more exotic content. Platforms that facilitate this, such as Twitter and Reddit, are growing rapidly, while those that actively inhibit this trend, like Facebook and Instagram, are finding it harder to grow.</p>

<p>It’s important to note that this migration is greatly enhanced when people can be anonymous. While this can lead to various PR issues, it also results in much stronger community formation. Tumblr, for example, had many extreme communities (e.g., furries), which ultimately contributed to its decline when it was sold to Yahoo. Many of these extreme communities boycotted the platform, viewing the transaction as overly “capitalist”.</p>

<div style="text-align: center;">
  <img src="/asset/images/platform_movement.png" alt="Platform movement" style="max-height: 600px; max-width: 90%; margin: 0 auto;" />
</div>
<p><br /></p>

<h2 id="the-next-phase-hyper-personalized-news-integration">The Next Phase: Hyper-Personalized News Integration</h2>

<p>These movements are nearing completion. The recommendation systems of these platforms are quite sophisticated, covering all content types and boasting vast user networks. The moat for most of these platforms is their current network effects.</p>

<p>I believe the next feature that most social media platforms will add will be hyper-personalized consolidation of events. It may resemble Twitter’s “trending” feature but will be highly personalized, potentially including local news like “Possible knife attack near you” or “Heavy traffic at Star Mall, possibly due to Christmas shopping.” This development will essentially push social media towards becoming a proper news platforms. Some Japanese social media platforms already publish news directly on their websites, which is an effective approach for platforms without powerful AI capabilities.</p>

<div style="text-align: center;">
  <img src="/asset/images/platform_ai_integration.png" alt="Platform AI feature" style="max-height: 600px; max-width: 90%; margin: 0 auto;" />
</div>
<p><br /></p>

<h2 id="the-final-frontier-vr-and-ai-convergence">The Final Frontier: VR and AI Convergence</h2>

<p>The final phase of the social media platforms will likely emerge when virtual reality (VR) and artificial intelligence (AI) converge. There are already many platforms, like character.ai, experiencing huge traffic from people interacting with custom AI avatars. However, achieving convincingly real interactions at a much lower cost will take time.</p>

<p>Meanwhile, VR needs significant improvement, especially in hardware. As we approach the 1nm limit in chip manufacturing, it’s unclear how these chips will continue to get faster. Perhaps a visionary individual will figure out the next breakthrough. Once we have super-powerful chips capable of processing realistic graphics and computer vision tasks, VR will be ready. This will also greatly enhance AR experiences. Check <a href="https://www.youtube.com/watch?v=YJg02ivYzSs" target="_blank">this</a></p>

<p>The merger of VR/AR and AI will not only result in complex human-AI relationships but also in the creation of entirely new realities. People will create their own fantasy worlds with unique rules and systems of governance.</p>

<div style="text-align: center;">
  <img src="/asset/images/fantastic_worlds.png" alt="Fantasy worlds" style="max-height: 600px; max-width: 90%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>I already see VR breaking geographical and cultural barriers, allowing people to interact as if they were next-door neighbors. I’ve participated in various discussions with complete strangers from different countries in VR, and it feels refreshing and superior to the asynchronous text-based communication we do everyday. However, I believe we still have some time (perhaps 5-6 years) before we fully realize this future.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In this post, I will explore how social media is evolving and what its next phase of evolution might be. I’ll focus on mainstream platforms, excluding message boards or local sites (like Niconico). I’m not an expert on this subject, just an avid reader of tech news, and I’m writing this as a quick analysis. Please consider this post in that context.]]></summary></entry><entry><title type="html">Database locked!</title><link href="https://piyushk.github.io/jekyll/update/2024/07/14/database-locked.html" rel="alternate" type="text/html" title="Database locked!" /><published>2024-07-14T20:44:32+05:30</published><updated>2024-07-14T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/07/14/database-locked</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/07/14/database-locked.html"><![CDATA[<p>I was using sqlite as a local lightweight database for a project and I ran into a weird error “Database is locked” (full trace <a href="/asset/images/database_locked.png" target="_blank">here
</a>). Since I had mostly worked with Postgres and MySQL in my previous projects, this error turned out to be slightly more tricky than what I had thought.</p>

<p>Since I was already using atomic transactions at all the places it is required plus given the fact that sqlite by default works with serializable isolation level, I assumed that the only thing that could be going wrong would be the timeout. That is, some process is holding the database lock for significantly longer and other processes, unable to write to the db, are throwing this error.</p>

<div style="text-align: center;">
  <img src="/asset/images/sqlite_timeout.png" alt="Discord bot" style="max-height: 600px; max-width: 70%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>I updated the timeout for every new connection that was created to 30 seconds. I was positive that no transaction is even remotely reaching this duration, but I was still getting the same “database locked” errors randomly. Goggling stuff I came around this blog <a href="https://fractaledmind.github.io/2023/12/11/sqlite-on-rails-improving-concurrency/" target="_blank">Fractaled Mind</a>. It mentions a very important piece of data that I think is missing from many online forums (and also is not very clear in the documentation).</p>

<div style="text-align: center;">
  <img src="/asset/images/sqlite_lock.png" alt="Discord bot" style="max-height: 600px; max-width: 90%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>The main issue is that the lock is not acquired when the transaction begins but when the write operation begins. This is very different from how other production databases work, and apart from other shortcomings of SQLite, this makes it a poor choice to be used in production.</p>
<div style="text-align: center;">
  <img src="/asset/images/transaction_lock.png" alt="Transaction Lock" style="max-height: 600px; max-width: 90%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>To fix this issue I made a simple context block that wrapped my atomic transactions, as shown below. The benefit of this approach is that you can independently make your transactions atomic/non-atomic.</p>

<div style="text-align: center;">
  <img src="/asset/images/sqlite_atomic_context.png" alt="Transaction Lock" style="max-height: 600px; max-width: 90%; margin: 0 auto;" />
</div>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I was using sqlite as a local lightweight database for a project and I ran into a weird error “Database is locked” (full trace here ). Since I had mostly worked with Postgres and MySQL in my previous projects, this error turned out to be slightly more tricky than what I had thought.]]></summary></entry><entry><title type="html">Designing a good enough Discord bot</title><link href="https://piyushk.github.io/jekyll/update/2024/07/06/discord-bot.html" rel="alternate" type="text/html" title="Designing a good enough Discord bot" /><published>2024-07-06T20:44:32+05:30</published><updated>2024-07-06T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/07/06/discord-bot</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/07/06/discord-bot.html"><![CDATA[<p>Recently, I worked on a Discord bot that people can use to generate AI images and videos. Here, I will share a quick overview of its system design.</p>

<div style="text-align: center;">
  <img src="/asset/images/discord_bot.png" alt="Discord bot" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>The Discord client and most of the FastAPI processes are quite fast, involving only some database operations. The GPU load has been offloaded to third-party APIs, and the results are received through webhooks. However, I noticed some slowdown when multiple users started using the bot simultaneously. Upon further investigation, I realized that only a couple of methods were the bottlenecks in the process, such as image_transform (which downloads ~20 images, resizes them, crops them, and finally uploads them). I shifted all these CPU-intensive methods to Lambda functions. This greatly improved the speed and decreased the load on the VM. Now that both CPU and GPU-intensive processes are highly scalable, the bot can easily handle thousands of users.</p>

<p>The bot structure was also made modular, using cogs for individual commands. Since Discord client threads have a connection timeout, most of the client calls returned immediately from the backend, and the tasks were added in the background. Sample code is shown below.</p>

<div style="text-align: center;">
  <img src="/asset/images/bot_fastapi_code.png" alt="Sample code" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>To scale this to millions of users, one would need to shard the client instance across multiple VMs, preferably in an ECS cluster. Similarly, the backend would need to be deployed in an ECS cluster.</p>

<p>As the bot created and stored numerous images and videos, we implemented a simple cron job to schedule S3 bucket cleanup. Whenever a file was created, we injected expiry_time metadata into it, which the cleanup job used to determine whether to delete the file or not.</p>

<div style="text-align: center;">
  <img src="/asset/images/lambda_cleanup.png" alt="Cleanup" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Recently, I worked on a Discord bot that people can use to generate AI images and videos. Here, I will share a quick overview of its system design.]]></summary></entry><entry><title type="html">The Next Evolution of Generative AI Tools</title><link href="https://piyushk.github.io/jekyll/update/2024/05/11/ai-tools.html" rel="alternate" type="text/html" title="The Next Evolution of Generative AI Tools" /><published>2024-05-11T20:44:32+05:30</published><updated>2024-05-11T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/05/11/ai-tools</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/05/11/ai-tools.html"><![CDATA[<p>This post explores the different tools in the generative AI landscape and what’s next.</p>

<div style="text-align: center;">
  <img src="/asset/images/tool_banner.webp" alt="AI Tools" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Release of the Stable Diffusion 1.5 kicked off a frenzy even among non-ai people as being able to generate images in a single click seemed just like magic. There were a couple of python scripts initially but the need for a proper tool led to SD webui/a1111 (made by a1111).</p>

<p><br /></p>
<h2 class="post-list-heading">Automatic1111</h2>

<p>A1111 was the first tool that got mass adoption in the AI community, in fact in the SD subreddit it was the only tool being used/discussed for a couple of months. Since there was a strong community effort behind it, a lot of extensions were developed for it and there were a lot PRs being made to improve features. People were experimenting with different things and sometimes the things that worked seemed counterintuitive to even the ML folks. I remember many people were asking for the “clip skip” feature as it is very useful for anime related model (models based on NAI) but in some Github discussions the maintainers were like “huh? why do you wanna skip clip layers, it makes no sense”. It is one of those things where we really need the entire community effort for it to work. Obviously now, it makes more sense and it is like a standard option in almost every tool. People were adding all kinds of extensions as well, like Ebsynth which would have been very tedious to setup if someone was using just Python scripts.</p>

<div style="text-align: center;">
  <img src="/asset/images/a1111.png" alt="A1111" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>A1111 is still used widely but it is in a slow decline and the main reason for it is that it was unable to capture the entire generative AI ecosystem. It was very good for tinkerers, those who want to add small features/extensions and decent for artists as they can experiment with a lot of settings but not really great for developers as there was a lot of inflexible code and the app is not built in modular way where different components can be added and removed to create a new “workflow”, like first do txt2img then img2vid then upscale.. all this was manual.</p>

<div style="text-align: center;">
  <img src="/asset/images/a1111_venn.png" alt="A1111" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p><br /></p>
<h2 class="post-list-heading">ComfyUI</h2>

<p>ComfyUI was a major step up from A1111, not only did it have a much slicker js interface it improved on a lot of issues that plagued A1111. It allowed for creation of proper workflows and it was much more developer friendly compared to A1111 which meant a LOT of custom nodes were (and still are) being built daily. Creating a workflow and being able to share it is an insanely powerful feature in itself that accelerated it’s adoption. You will find many people (including myself) opening tickets inside new ML repos asking for them to port it to ComfyUI. In a sense, it managed to capture both the tinkerers and developers completely, but there is still a large part of the artist community that needs to be pulled into the AI art revolution. One other drawback of Comfy is that since it is managed in a very decentralised way, breaking of flows is common. For e.g. a couple of days ago the Derfuu Integer node was removed from the Comfy Manager (that is managed by some other individual) and it broke a lot of workflows instantly. Also, there are a lot of nodes that are basically direct wrappers on top of the diffusers code that adds to the inefficiencies and there are no stable releases (as everyone is making their own release).</p>

<div style="text-align: center;">
  <img src="/asset/images/comfyui_ss.png" alt="ComfyUI" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<div style="text-align: center;">
  <img src="/asset/images/comfy_venn.png" alt="ComfyUI" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Many companies and startups have started cloning the node based structure of ComfyUI and making incremental adjustments to it. Also there are other tools similar to Comfy but with a very robust codebase, like InvokeAI. I think because of how InvokeAI is re-imagining the node based tools, it can be very powerful and may become the dominant player. As it also provides a minimal interface for artists who are not very familiar with the node based structure.</p>

<p>Dough was an attempt by us (Banodoco) to provide a set steps for video generation. Images &gt; Customize Images &gt; Arrange in shots &gt; Generate video. Although it is very good for video generation use cases, it fails in roping in the devs and tinkerers, also having limited options for generation limits artist (although that will be fixed soon!). Leaving out even one group has a big impact on the adoption, but it is something that we will fix in it’s next iteration.</p>

<div style="text-align: center;">
  <img src="/asset/images/dough_venn.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p><br /></p>
<h2 class="post-list-heading">What's Next?</h2>

<p>I believe the companies banking on ComfyUI ecosystem will have a tough time in the near future because it’s adoption will slowly decrease. Just like A1111 hosting services are far and in-between, ComfyUI workflow hostings will vanish. The next iteration of tools (or tool) will have the following characteristic</p>
<ol>
  <li>It will be open source and community-driven but still a focused effort and would have some kind of community-driven gatekeeping for what to let in (in terms of PRs/functionalities)</li>
  <li>Stable code releases including all the bells and whistles</li>
  <li>A UI that allows creating processes on top of the workflows</li>
  <li>It is cohesive and feels like a native app, if people want they go into the noodle-graph land or directly into the code but overall the app provides multiple levels of abstraction</li>
</ol>

<p>What this tool will look like? I don’t know but hopefully we will have something very close to it pretty soon!</p>

<div style="text-align: center;">
  <img src="/asset/images/next_gen_venn.png" alt="Next gen tool" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This post explores the different tools in the generative AI landscape and what’s next.]]></summary></entry><entry><title type="html">Dough: Low level design</title><link href="https://piyushk.github.io/jekyll/update/2024/05/11/dough-lld.html" rel="alternate" type="text/html" title="Dough: Low level design" /><published>2024-05-11T20:44:32+05:30</published><updated>2024-05-11T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/05/11/dough-lld</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/05/11/dough-lld.html"><![CDATA[<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/dough_0.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>The AI art tool that I have been working for quite a while now is finally released in beta! You can try it here - <a href="https://github.com/banodoco/Dough">Dough Beta</a> . Pom provided a lot of inputs and tirelessly worked with the tool to refine the layout/functioning. I was handling most of the technical aspects and in this post I will share a low level design of Dough.</p>

<p>Firstly, from the inception I tried to keep Dough as modular as possible. Different components and integrations can be swapped easily and developed independently. At the center of the app is the core Streamlit UI and related utility functions. These interact with the data layer, that is essentially the backend, that can be easily plugged into multiple backends. In fact we had different backends for the local and hosted version (obviously!). Different backends can have their own databases and logic for maintaining users and payments.</p>

<div style="text-align: center;">
  <img src="/asset/images/dough_1.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>As Streamlit refreshes the entire app (every single line!) anytime a state change occurs it becomes super hard to integrate APIs with it as we can’t have the app calling 30 different APIs on a single dropdown click. A simple solution for this was to cache as much as possible. I created a simple cache decorator that was placed on top of the DataRepo (entrypoint to the data layer). This way it could cache any backend connected to it, local or hosted. Below you can see a simple function inside the decorator, which first checks the cache and if the data is not found then it hits the API.</p>

<div style="text-align: center;">
  <img src="/asset/images/dough_2.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/dough_3.png" alt="Dough" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Now having just used the A1111 there was an obvious issue, people wanted to host it but it wasn’t built for a new backend integration, the UI layer was tighly coupled with the ML layer and thus many startups trying to deploy it were deploying it in a completely new container everytime. This meant a lot of GPU time was wasted where people were just playing with the UI. Also everything was synchronous, which means that the UI will be blocked until the generation completes. We planned on making the operations asynchronous, which means the user is free to interact with the UI while the generation runs in the background. To solve both of these issues I made a modular ML layer and a background runner that schedules the tasks and updates the generation status. Having this modularity means anyone can plug in their own ML processor, we used to have third-party APIs but were able to quickly add ComfyUI support for local generations because of this modularity.</p>

<div style="text-align: center;">
  <img src="/asset/images/dough_4.png" alt="Notification History" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>There are still a lot of improvements to be done and features to be added. The technical side is just one aspect of the app, Pom’s push for perfection and insights in UX/functionality has been monumental in it’s development. We plan on making a much slicker and fluid version in a JS framework very soon!</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Artifact: social with news &amp;gt; news with social</title><link href="https://piyushk.github.io/jekyll/update/2024/01/13/Artifact.html" rel="alternate" type="text/html" title="Artifact: social with news &amp;gt; news with social" /><published>2024-01-13T20:44:32+05:30</published><updated>2024-01-13T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2024/01/13/Artifact</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2024/01/13/Artifact.html"><![CDATA[<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/artifact_closing.png" alt="Artifact Closing" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>As someone who has always been interested in the consumer social sector, I have been following Artifact’s progress from its launch. I was intrigued by the idea of mixing news with powerful ML recommendations, especially coming from people who understood this segment better than anyone. One example that Kevin Systrom gave in one of his interviews was that TikTok mixed videos with the most powerful ML recommender system and it completely changed the video consumption habits of the people. He believed that the new generation of products would have the traditional delivery mechanism but would be delivered in a highly personalized way. That’s very smart considering in the case of news the more polarizing it is the faster it spreads, and in the case of personalization, it can be made to appeal to different niches.</p>

<p>Here are some of my thoughts on what I think worked for them and what didn’t. First of all, what worked for them? I think they had a very strong sense of app features that give dopamine hits, e.g. when you share a link to some news article and someone opens it, you get a notification that someone viewed the link you opened. They added a very nice feature, the “clickbait or not” flag. The app was super smooth and never felt like an MVP of any kind.</p>

<p>Now for the things they got wrong. First, I think they were not able to nail the virality aspect of the news. Here, is a screenshot of notifications of Artifact vs Inshorts (a popular news app in India). I think their algorithm optimized for utility over virality. In the beginning, I feel they should have covered some sensational topics to get some eyeballs and have their comment sections filled with discussions. The main advantage of this would have been that they would have increased their install base.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/artifact_notifications.png" alt="Notification History" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>Secondly, I feel one major issue with them was that they couldn’t distinguish themselves from the competition. News summary? I have already been using an app for that for the past 8 years. Comments/reactions in the news? there are tons of apps for that. There was simply nothing that made them stand out. I am a tech enthusiast therefore I had the app installed, but no one else in my circle was using it. Thirdly, I think they took the wrong approach by trying to mix social aspects in a text news app when there are already text social media with news aspects (Twitter). In social media, if you like a dance video, the influencer can create 10 more videos in a day to keep you glued to the screen but it’s not the case with news, you can’t make 10 more news similar to what I like out of thin air. This means the ‘content’ generation is a huge problem in news and that’s why almost all the major players rely on sensationalism/dramatization.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/jeremy_corbell.png" alt="Clickbait" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>I believe one of the mistakes they made was to fundamentally confuse the nature of news consumption in today’s world. First, is of course the consumption of sensational news stories but the second is creating engaging content around it. Social media can magically create content around the same news and have you watching the same thing for hours, how? For. e.g. Biden slips while climbing a staircase. In a news app, this would be just one notification (maybe not even that) but on a social media platform, you can have hours of content on it from memes to critic videos. One other reason why social media dominates the news space more than the news itself is because that is where news gets created (not all but many).</p>

<p>I feel people will naturally gravitate towards the niche they like but they won’t consume text content with ML recommendation. They feel much more comfortable watching a random dude dramatize the whole thing and create an echo chamber for them. Since the mainstream media has lost all its credibility people turn to influencers (who are in many cases subject experts) to seek news and that’s why we see the rise of YouTube channels such as “China uncensored”. Also, in my opinion, news, however engaging, is not something that people will consume continuously and that’s also why they prefer social media because it gives them a mixture of content to consume news, sports, dumb fun, gaming, etc.. Overall, I think it was somewhat inevitable that Artifact was going to have a tough time with its initial concept as its founders said that they were unsure during its development phase if it would work or not.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Deploying a Streamlit app on production</title><link href="https://piyushk.github.io/jekyll/update/2023/12/30/streamlit-system-design.html" rel="alternate" type="text/html" title="Deploying a Streamlit app on production" /><published>2023-12-30T20:44:32+05:30</published><updated>2023-12-30T20:44:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2023/12/30/streamlit-system-design</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2023/12/30/streamlit-system-design.html"><![CDATA[<p>I have been working on an open source AI video generator for quite some time now. Though it’s a simple tool, it being built in Streamlit posed some challenges both in it’s internal structuring and deployment. Here, I will share some of the problems faced by me and how I solved them.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/app_screenshot.png" alt="App Screenshot" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
  <figcaption>App Screenshot (WIP)</figcaption>
</div>
<p><br /></p>

<p>The very first thing I did to make the app scalable was to deploy it on an ECS cluster, which depending on the load will auto-scale the number of containers running. The problem with this approach is that AWS load balancer auto routes the incoming requests to containers with low load whenever the number of containers changes. Since Streamlit is server side rendered and it maintains the state locally (on the server), this switch will kill the app functioning, especially in our case since we are keeping a large amount of data in the Streamlit session state for quick reloads. After some searching I found that AWS provides the option to enable sticky sessions in ecs target group alb which maintains the user’s connection to a specific ECS container (connection is identified by browser cookies). This setting can be enabled by adding the following terraform code inside aws_alb_target_group.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/sticky_session.png" alt="Sticky session" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
</div>
<p><br /></p>

<p>After this the problem that we encountered a lot with the app was speed. Since, Streamlit is somewhat like Flutter in the regards that it refreshes the entire app on every little change, our app was getting very slow, since we were making quite a few API calls to the backend. The solution to this required changing the deployment options and the internal app structure. In the case of deployment I noticed that the DNS resolution and the TLS handshake were taking a significant amount of time. Since the user will never see any of the https calls (Streamlit directly sends the rendered page to the end user) I can just use http instead of https. Also instead of going over the internet and doing DNS resolution, I created an internal private DNS which the Streamlit app can use to quickly connect to the backend. This more than doubled the API interaction speed.</p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/tls.webp" alt="API timing" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
  <figcaption>API Timings</figcaption>
</div>
<p><br /></p>

<p><br /></p>
<div style="text-align: center;">
  <img src="/asset/images/ai_app_design.png" alt="High level system design" style="max-height: 600px; max-width: 100%; margin: 0 auto;" />
  <figcaption>High level system arch</figcaption>
</div>
<p><br /></p>

<p>Now on the app side I tried to cache as many data models as I could. We built a simple wrapper on top of our API repository, which cached the API results in the session state. The caching logic we used was very simple, we stored the API data as they came and whenever a particular model was altered (updated/deleted) we refreshed all the related entities. This again gave us a significant speed boost. We are still looking for ways to make it even faster, but not having a proper state management solution in Streamlit creates some challenges.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I have been working on an open source AI video generator for quite some time now. Though it’s a simple tool, it being built in Streamlit posed some challenges both in it’s internal structuring and deployment. Here, I will share some of the problems faced by me and how I solved them.]]></summary></entry><entry><title type="html">Generative AI: A semi-technical look</title><link href="https://piyushk.github.io/jekyll/update/2023/07/19/generative_ai.html" rel="alternate" type="text/html" title="Generative AI: A semi-technical look" /><published>2023-07-19T21:54:32+05:30</published><updated>2023-07-19T21:54:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2023/07/19/generative_ai</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2023/07/19/generative_ai.html"><![CDATA[<p>I have been quite busy since my last post. In this post, I will cover my experience with generative AI and what I think about it from a technical perspective being a software developer. I worked on multiple personal projects, some complete, some incomplete, and some still in progress. I will start with the LLMs as it was ChatGPT that took the internet by storm. Though AI was here long ago, it was the first time that people could use it naturally with the chat interface. Having opened their API to the public everyone started building wrappers on top of it. To me as well it looked like the right opportunity to ride the AI wave or at least make something worthwhile. So, I started with the problem most software engineers are troubled by, coding. I wanted to make a program that will automatically write the code based on your existing code base and development practices. This is a high-level overview of what I thought would work.</p>

<p><img src="/asset/images/auto_code.png" alt="Auto code genrator" /></p>

<p>And this was the result</p>

<video width="100%" preload="auto" muted="" controls="" style="margin: auto;">
    <source src="/asset/videos/code_generator.mp4" type="video/mp4" />
</video>
<p><br /></p>

<p>The results were not that great. Although ChatGPT was good at giving small code snippets, writing the entire code seemed very far fetched (I believe it still is). The chaining of multiple commands utilizing CoT and ReAct added more to the problem as a single deviation/error at one of the steps compounded heavily till the program reached the end. I tried to minimize the error in multiple ways but had no luck so I gave up. I looked at other impressive demos on Twitter but none had a good working product and everyone had the same issue. I believe even with GPT-5 coding the entire codebase is an extremely far fetched scenario. Maybe this was the major breakthrough that we would see even in the next 5-7 years.</p>

<p>From LLMs I switched my focus to image generation as it impressed me much more than the LLMs. I tried the base SD in the very beginning and it was super bad but when I tried Midjourney I was just blown away. I have been an admirer of art (especially fantasy art) for a long time, and seeing a computer generate such amazing images in just seconds was a surreal experience. I tried a bunch of custom models and quickly created a website where people can generate images. I also tried a bunch of other things to see what sticks, but the pace of development in AI space was so fast, with players picking millions in funding that it was almost impossible to keep up. A simple example would be Prompthunt and similar sites, which started by selling prompts but their business model was soon invalidated when both Stable Diffusion and Midjourney launched their prompt predictor tool (Clip and /describe). Here’s the site that I made, I am still experimenting with different things on this and hoping I find the right hook to monetize this somehow.</p>

<p><br />
<img src="/asset/images/promptpedia.png" alt="PromptPedia" />
<br />
<br /></p>

<p>Custom models generated amazing results while Midjourney is in a league of its own. Midjourney reaching $1 billion ARR shows just how much of a big deal this was. But one major issue I felt with image generation was the loss of control and I think text-to-image is not the right way to go about it. A different way to think about this is the data filling, we provide some text which is a couple of KBs, and the AI model generates an image that is a couple of MBs. This thousandfold data creation is largely driven by the model’s training and there is very little control we have over it. For example, I wanted to make an image of a fantasy land in a dark setting with rocks flying. I already had its image by some artist but wanted to make it even better. No matter what I did it was almost impossible to get what I want, the only solution that could work in these scenarios is to draw the image from scratch by hand. AI image models are still a black box and though you can still achieve a lot by proper prompt engineering and other control techniques (like controlnet, image-to-image, and parameter tuning) it’s nowhere close to artists creating art from scratch.</p>

<p>I next started experimenting with AI video generation. At the time there were just some basic demos out there by Runway but nothing concrete. A lot of people on Reddit and Twitter were generating videos by converting every single frame than removing the noise artifacts. But in my opinion, these simple restyling can be better done by static programs like EBsynth. I tried a lot of different things, the main ones being multi controlnet and image-to-image at different settings but couldn’t get a good result. The only way that seemed to work was to train an SD model on both character and style of the particular character that you wish to transform and then apply multi-controlnet along with the custom model that you created. Of course, this makes very little practical sense and thus still I have no good way to make this work. Check one of the results that I generated below.</p>

<p><br /></p>
<video width="80%" preload="auto" muted="" controls="" style="margin: auto;">
    <source src="/asset/videos/ai_vid_to_vid.mp4" type="video/mp4" />
</video>
<p><br /></p>

<p>I believe there is still significant research and development left in the AI space. Though the results are impressive and can perform very well on small tasks they are still far from practical use cases and nowhere near what the AI doomsayer would have us believe. I will keep experimenting with this stuff and update it here. The quote from Yann LeCun, Chief AI scientist at Meta summarizes this perfectly.</p>

<p><br />
<img src="/asset/images/yann.png" alt="Yann LeCun Tweet" />
<br /></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I have been quite busy since my last post. In this post, I will cover my experience with generative AI and what I think about it from a technical perspective being a software developer. I worked on multiple personal projects, some complete, some incomplete, and some still in progress. I will start with the LLMs as it was ChatGPT that took the internet by storm. Though AI was here long ago, it was the first time that people could use it naturally with the chat interface. Having opened their API to the public everyone started building wrappers on top of it. To me as well it looked like the right opportunity to ride the AI wave or at least make something worthwhile. So, I started with the problem most software engineers are troubled by, coding. I wanted to make a program that will automatically write the code based on your existing code base and development practices. This is a high-level overview of what I thought would work.]]></summary></entry><entry><title type="html">Thoughts on the metaverse</title><link href="https://piyushk.github.io/jekyll/update/2023/04/17/metaverse.html" rel="alternate" type="text/html" title="Thoughts on the metaverse" /><published>2023-04-17T21:54:32+05:30</published><updated>2023-04-17T21:54:32+05:30</updated><id>https://piyushk.github.io/jekyll/update/2023/04/17/metaverse</id><content type="html" xml:base="https://piyushk.github.io/jekyll/update/2023/04/17/metaverse.html"><![CDATA[<p>I have been using the Oculus Quest 2, on and off for about 1 year now. The first time I used it I was blown away. I thought to myself, this is definitely the future. Weirdly enough, when I discussed it with my friends and even the reviews online, no one particularly seems to enjoy it. “It’s a poorly designed gaming system” - this is the typical response that I got. Later in this post, I will delve into potential enhancements for broader adoption, as well as fundamental paradigm shifts that may be necessary.</p>

<p>The virtual and augmented reality (AR/VR) industry has plummeted from investors’ radar due to lackluster sales figures of Meta and with highly anticipated startups like Magic Leap failing to meet expectations by a significant margin. Top tech personalities like Elon Musk have also said that “I don’t see someone strapping a friggin’ screen to their face all day”. John Carmack who was in charge of building the metaverse left Meta saying that the pace of innovation is slow.</p>

<p><br /></p>
<video width="80%" preload="auto" muted="" controls="" style="margin: auto;">
    <source src="/asset/videos/bigscreen_ss.mp4" type="video/mp4" />
</video>
<p><br /></p>

<p>This is a short screen capture that I did in the VR and it felt like I was really there. The discussions in VR are very different from what one experiences in group chats or group calls. There really is an immersive feel to it. But it comes at a cost, when you enter the metaverse you become completely disconnected from the actual world. So unlike on a group video call, you can’t quickly share/access your files on your computer/mobile or the internet. You can’t hear your mom yelling for you to come down. For a little amount of immersion, it feels that we sacrifice a large amount of control and awareness. This can be both good and bad depending on the circumstances, but ideally, it’s good for things that require less control. For e.g. watching movies, sitting in a group and just discussing random topics, or playing a game where all the controls are already present in the VR. For most of the day-to-day work, it would be difficult to migrate to VR.</p>

<p>One other issue is that very few of your friends are in it. Even if they are, there are very few things to be done. We already have MMORPGs and similar games, where your network is present and the ‘immersive’ experience doesn’t make much difference. As stated in “The cold start problem”, the goal should be to first build an atomic network and then grow from there. I believe Meta had the right idea, of focusing on creating VR-first experiences. It can be a game but mostly has to be something different. Maybe it’s a field trip to an alien planet with your friends, maybe it’s an escape room, or a VR documentary. I believe BigScreenVR has found its core group of users (like me), who tune in every now and then. What’s their secret sauce? They let anyone host a private room and share any video. So you could in ‘theory’ have ‘some’ people screening the latest movies and you get to watch it from a perfect spot in a grand theatre without a kid crying in the back.</p>

<p>User experience is the other thing that creates a lot of friction. First, you have to put on the clunky headsets in position then turn them on then wait for them to boot. If in between you have to go to the bathroom, you have to take the entire thing off (headsets, controllers, charging cables, headphones, and wires) and then put it back on when you come back. The headset should be small, super quick to boot (or maybe should plug in directly to your laptop), have a long battery life or charges through the laptop connection, and has just simple gloves for hand tracking (no turning on or battery required for those). I believe one other limitation is the technology, current graphics look like they are from the PS2 era. As Apple has been always great at striking the balance between tech and user experience, I believe they have the most chance of nailing this correctly. We will have to wait and see what Apple unveils in June.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I have been using the Oculus Quest 2, on and off for about 1 year now. The first time I used it I was blown away. I thought to myself, this is definitely the future. Weirdly enough, when I discussed it with my friends and even the reviews online, no one particularly seems to enjoy it. “It’s a poorly designed gaming system” - this is the typical response that I got. Later in this post, I will delve into potential enhancements for broader adoption, as well as fundamental paradigm shifts that may be necessary.]]></summary></entry></feed>